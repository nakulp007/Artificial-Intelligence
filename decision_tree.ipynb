{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Decision Tree Learning\n",
    "\n",
    "In this assignment, you will work with a class of reinforcement learning agents called decision trees to attempt to classify features according to some decision boundary.\n",
    "\n",
    "\n",
    "This assignment is due on T-Square on November 3 by 9:35 AM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction:\n",
    "-------\n",
    "\n",
    "For this assignment we're going to need an explicit way to make structured decisions. The following is a decision node- a class representing some atomic choice in a binary decision graph. It can represent a class label (i.e. a final decision) or a binary decision to guide the us through a flow-chart to arrive at a decision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DecisionNode():\n",
    "\n",
    "    def __init__(self, left, right, decision_function,class_label=None):\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "        self.decision_function = decision_function\n",
    "        self.class_label = class_label\n",
    "\n",
    "    def decide(self, feature):\n",
    "        if self.class_label != None:\n",
    "            return self.class_label\n",
    "\n",
    "        return self.left.decide(feature) if self.decision_function(feature) else self.right.decide(feature)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Warmup: Building a tree by hand\n",
    "--------\n",
    "20 pts.\n",
    "\n",
    "In the below code block, construct a tree of decision nodes by hand in order to classify the data below. Select tests to be as small as possible (in terms of attributes), breaking ties among tests with the same number of attributes by selecting the one that classifies the greatest number of examples correctly. If multiple tests have the same number of attributes and classift the same number of examples, then break the tie using attributes with lower index numbers (e.g. select $A_1$ over $A_2$)\n",
    "\n",
    "| Datum  | $A_1$ | $A_2$ | $A_3$ | $A_4$ |  y  |\n",
    "| -------| :---: | :---: | :---: | :---: | ---:|\n",
    "| $x_1$  |   1   |   0   |   0   |   0   |  1  |\n",
    "| $x_2$  |   1   |   0   |   1   |   1   |  1  |\n",
    "| $x_3$  |   0   |   1   |   0   |   0   |  1  |\n",
    "| $x_4$  |   0   |   1   |   1   |   0   |  0  |\n",
    "| $x_5$  |   1   |   1   |   0   |   1   |  1  |\n",
    "| $x_6$  |   0   |   1   |   0   |   1   |  0  |\n",
    "| $x_7$  |   0   |   0   |   1   |   1   |  1  |\n",
    "| $x_8$  |   0   |   0   |   1   |   0   |  0  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "examples = [[1,0,0,0],\n",
    "            [1,0,1,1],\n",
    "            [0,1,0,0],\n",
    "            [0,1,1,0],\n",
    "            [1,1,0,1],\n",
    "            [0,1,0,1],\n",
    "            [0,0,1,1],\n",
    "            [0,0,1,0]]\n",
    "\n",
    "classes = [1,1,1,0,1,0,1,0]\n",
    "\n",
    "# Constructing nodes one at a time,\n",
    "# build a decision tree as specified above.\n",
    "# There exists a correct tree with less than 6 nodes.\n",
    "\n",
    "def A1DFunction(feature):\n",
    "    if feature[0] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def A2DFunction(feature):\n",
    "    if feature[1] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def A3DFunction(feature):\n",
    "    if feature[2] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "def A4DFunction(feature):\n",
    "    if feature[3] == 1:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "TDNode = DecisionNode(None, None, None, 1)\n",
    "FDNode = DecisionNode(None, None, None, 0)\n",
    "A42DNode = DecisionNode(FDNode, TDNode, A4DFunction)\n",
    "A3DNode = DecisionNode(FDNode, A42DNode, A3DFunction)\n",
    "A4DNode = DecisionNode(TDNode, FDNode, A4DFunction)\n",
    "A2DNode = DecisionNode(A3DNode, A4DNode, A2DFunction)\n",
    "\n",
    "decision_tree_root = DecisionNode(TDNode, A2DNode, A1DFunction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1b: Validation\n",
    "--------\n",
    "\n",
    "Now that we have a decision tree, we're going to need some way to evaluate its performance. In most cases we'd reserve a portion of the training data for evaluation, or use cross validation, bot for now let's just see how your tree does on the provided examples. In the stubbed out code below, fill out the methods to compute accuracy, precision, recall, and the confusion matrix for your classifier output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "[[5, 0], [0, 3]]\n"
     ]
    }
   ],
   "source": [
    "def confusion_matrix(classifier_output, true_labels):\n",
    "    #TODO output should be [[true_positive, false_negative], [false_positive, true_negative]]\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "    true_negative = 0\n",
    "    for x in range(0, len(classifier_output)):\n",
    "        if classifier_output[x] == 1 and true_labels[x] == 1:\n",
    "            true_positive += 1\n",
    "        elif classifier_output[x] == 0 and true_labels[x] == 0:\n",
    "            true_negative += 1\n",
    "        elif classifier_output[x] == 1 and true_labels[x] == 0:\n",
    "            false_positive += 1\n",
    "        elif classifier_output[x] == 0 and true_labels[x] == 1:\n",
    "            false_negative += 1\n",
    "    return [[true_positive, false_negative], [false_positive, true_negative]]\n",
    "\n",
    "def precision(classifier_output, true_labels):\n",
    "    #TODO precision is measured as: true_positive/ (true_positive + false_positive)\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    for x in range(0, len(classifier_output)):\n",
    "        if classifier_output[x] == 1 and true_labels[x] == 1:\n",
    "            true_positive += 1\n",
    "        elif classifier_output[x] == 1 and true_labels[x] == 0:\n",
    "            false_positive += 1\n",
    "    if true_positive + false_positive == 0:\n",
    "        return 0\n",
    "    return (true_positive/ float(true_positive + false_positive))\n",
    "\n",
    "def recall(classifier_output, true_labels):\n",
    "    #TODO: recall is measured as: true_positive/ (true_positive + false_negative)\n",
    "    true_positive = 0\n",
    "    false_negative = 0\n",
    "    for x in range(0, len(classifier_output)):\n",
    "        if classifier_output[x] == 1 and true_labels[x] == 1:\n",
    "            true_positive += 1\n",
    "        elif classifier_output[x] == 0 and true_labels[x] == 1:\n",
    "            false_negative += 1\n",
    "    if true_positive + false_negative == 0:\n",
    "        return 0\n",
    "    return (true_positive/ float((true_positive + false_negative)))\n",
    "\n",
    "def accuracy(classifier_output, true_labels):\n",
    "    #TODO accuracy is measured as:  correct_classifications / total_number_examples\n",
    "    correct_classifications = 0\n",
    "    for x in range(0, len(classifier_output)):\n",
    "        if classifier_output[x] == true_labels[x]:\n",
    "            correct_classifications += 1\n",
    "    if true_labels == 0:\n",
    "        return 0\n",
    "    return (correct_classifications/float(len(true_labels)))\n",
    "\n",
    "classifier_output = [decision_tree_root.decide(example) for example in examples]\n",
    "\n",
    "# Make sure your hand-built tree is 100% accurate.\n",
    "p1_accuracy = accuracy( classifier_output, classes )\n",
    "p1_precision = precision(classifier_output, classes)\n",
    "p1_recall = recall(classifier_output, classes)\n",
    "p1_confusion_matrix = confusion_matrix(classifier_output, classes)\n",
    "\n",
    "print p1_accuracy\n",
    "print p1_precision\n",
    "print p1_recall\n",
    "print p1_confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Decision Tree Learning\n",
    "-------\n",
    "40 pts.\n",
    "\n",
    "As the number of examples we have grows, it rapidly becomes impractical to build these trees by hand, so it becomes necessary to specify a procedure by which we can automagically construct these trees.\n",
    "\n",
    "For starters, let's consider the following algorithm (a variation of C4.5) for the construction of a decision tree from a given set of examples:\n",
    "\n",
    "    1) Check for base cases: \n",
    "         a)If all elements of a list are of the same class, return a leaf node with the appropriate class label.\n",
    "         b)If a specified depth limit is reached, return a leaf labeled with the most frequent class.\n",
    "\n",
    "    2) For each attribute alpha: evaluate the normalized information gain gained by splitting on alpha\n",
    "\n",
    "    3) Let alpha_best be the attribute with the highest normalized information gain\n",
    "\n",
    "    4) Create a decision node that splits on alpha_best\n",
    "\n",
    "    5) Recur on the sublists obtained by splitting on alpha_best, and add those nodes as children of node\n",
    "\n",
    "In the \\_\\_build_tree\\__ method below implement the above algorithm. In the \"classify\" method below, write a function to produce classifications for a list of features once your decision tree has been build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def entropy(class_vector):\n",
    "    import math\n",
    "    # TODO: Compute the Shannon entropy for a vector of classes\n",
    "    # Note: Classes will be given as either a 0 or a 1.\n",
    "    p = class_vector.count(1)\n",
    "    n = class_vector.count(0)\n",
    "    t = len(class_vector)\n",
    "    if t == 0:\n",
    "        return 0\n",
    "    if (p/float(t)) != 0.0 and (n/float(t)) != 0.0:\n",
    "        return (-( (p/float(t))*math.log((p/float(t)), 2)  )  -( (n/float(t))*math.log((n/float(t)), 2)  ))\n",
    "    elif (p/float(t)) != 0.0 and (n/float(t)) == 0.0:\n",
    "        return (-( (p/float(t))*math.log((p/float(t)), 2)  )  -( 0  ))\n",
    "    elif (p/float(t)) == 0.0 and (n/float(t)) != 0.0:\n",
    "        return (-( 0 )  -( (n/float(t))*math.log((n/float(t)), 2)  ))\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def information_gain(previous_classes, current_classes ):\n",
    "    # TODO: Implement information gain\n",
    "    def expected_entropy(previous_classes, current_classes):\n",
    "        prevP = previous_classes.count(1)\n",
    "        prevN = previous_classes.count(0)\n",
    "        prevT = len(previous_classes)\n",
    "        currP = current_classes.count(1)\n",
    "        currN = current_classes.count(0)\n",
    "        currT = len(current_classes)\n",
    "        notGiven_classes = [0]*(prevN - currN)\n",
    "        notGiven_classes = notGiven_classes.__add__([1]*(prevP - currP))\n",
    "        ngP = notGiven_classes.count(1)\n",
    "        ngN = notGiven_classes.count(0)\n",
    "        ngT = len(notGiven_classes)\n",
    "        return ( ((currT/float(prevT))*(entropy(current_classes))) + ((ngT/float(prevT))*(entropy(notGiven_classes))) )\n",
    "    return (entropy(previous_classes) - expected_entropy(previous_classes,current_classes))\n",
    "\n",
    "\n",
    "class DecisionTree():\n",
    "\n",
    "    def __init__(self, depth_limit=float('inf')):\n",
    "        self.root = None\n",
    "        self.depth_limit = depth_limit\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        self.root = self.__build_tree__(features, classes)\n",
    "\n",
    "    def __build_tree__(self, features, classes, depth=0):\n",
    "        import copy\n",
    "        import numpy as np\n",
    "        #TODO Implement the algorithm as specified above\n",
    "        #print \"depth is: \" + str(depth)\n",
    "        #print \"features: \" + str(features)\n",
    "        #print \"classes: \" + str(classes)\n",
    "\n",
    "        #print \"features size: \" + str(len(features)) + \"x\" + str(len(features[0]))\n",
    "        #print \"classes size: \" + str(len(classes))\n",
    "\n",
    "        TDNode = DecisionNode(None, None, None, 1)\n",
    "        FDNode = DecisionNode(None, None, None, 0)\n",
    "        \"\"\"\n",
    "        1) Check for base cases:\n",
    "            a)If all elements of a list are of the same class, return a leaf node with the appropriate class label.\n",
    "            b)If a specified depth limit is reached, return a leaf labeled with the most frequent class.\n",
    "            My addition to algo: If there are no attributes left, return most frequent class\n",
    "        \"\"\"\n",
    "        if classes == [0]*len(classes):\n",
    "            return FDNode\n",
    "        elif classes == [1]*len(classes):\n",
    "            return TDNode\n",
    "\n",
    "        #depth percent of attributes\n",
    "        if len(features[0]) == 0 or (self.depth_limit != float('inf') and depth == self.depth_limit):\n",
    "            if classes.count(1) > classes.count(0):\n",
    "                return TDNode\n",
    "            else:\n",
    "                return FDNode\n",
    "\n",
    "        \"\"\"\n",
    "        2) For each attribute alpha: evaluate the normalized information gain gained by splitting on alpha\n",
    "        \"\"\"\n",
    "        bestAttributeIndex = 0\n",
    "        bestInfoGain = 0.0\n",
    "        #go through each attribute\n",
    "        for x in range(0, len(features[0])):\n",
    "            #attribute column\n",
    "            a = [row[x] for row in features]\n",
    "            #split on true\n",
    "            #all final true false values where attrubute a was true\n",
    "            curr_classes = []\n",
    "            for y in range(0, len(a)):\n",
    "                if a[y] > 0: #== 1:\n",
    "                    curr_classes.append(classes[y])\n",
    "\n",
    "            infoGain = information_gain(classes, curr_classes)\n",
    "            if infoGain > bestInfoGain:\n",
    "                bestAttributeIndex = x\n",
    "                bestInfoGain = infoGain\n",
    "\n",
    "        \"\"\"\n",
    "        3) Let alpha_best be the attribute with the highest normalized information gain\n",
    "        \"\"\"\n",
    "        alpha_best = [row[bestAttributeIndex] for row in features]\n",
    "\n",
    "        \"\"\"\n",
    "        4) Create a decision node that splits on alpha_best\n",
    "        \"\"\"\n",
    "        #remove best attribute column\n",
    "        new_features = copy.deepcopy(features)\n",
    "        for x in range(0, len(new_features)):\n",
    "            new_features[x] = np.delete(new_features[x], bestAttributeIndex)\n",
    "\n",
    "        l_features = []\n",
    "        l_classes = []\n",
    "        r_features = []\n",
    "        r_classes = []\n",
    "        #split cases that will go left and ritht, left will cases where alpha_best will be true\n",
    "        for x in range(0, len(alpha_best)):\n",
    "            if alpha_best[x] > 0: #== 1:\n",
    "                l_features.append(new_features[x])\n",
    "                l_classes.append(classes[x]) #.add\n",
    "            else:\n",
    "                r_features.append(new_features[x])\n",
    "                r_classes.append(classes[x]) #.add\n",
    "\n",
    "        def RootDFunction(feature):\n",
    "            if feature[bestAttributeIndex] > 0:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        #print \"about to build a tree with l features: \" + str(l_features)\n",
    "        #print \"about to build a tree with l class: \" + str(l_classes)\n",
    "        #print \"about to build a tree with r reatures: \" + str(r_features)\n",
    "        #print \"about to build a tree with r class: \" + str(r_classes)\n",
    "        #dn left, right, decisionfunction, class\n",
    "        root_node = DecisionNode(self.__build_tree__(l_features, l_classes, depth+1), self.__build_tree__(r_features, r_classes, depth+1), RootDFunction)\n",
    "\n",
    "        \"\"\"\n",
    "        5) Recur on the sublists obtained by splitting on alpha_best, and add those nodes as children of node\n",
    "        \"\"\"\n",
    "        # already doing that above\n",
    "\n",
    "        return root_node\n",
    "\n",
    "    def classify(self, features):\n",
    "        #TODO Use a fitted tree to classify a list of feature vectors\n",
    "        # Your output should be a list of class labels (either 0 or 1)\n",
    "        return [self.root.decide(feature) for feature in features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2b: Validation\n",
    "--------\n",
    "\n",
    "For this part of the assignment we're going to use a relatively simple dataset (banknote authentication, found in 'part_2_data.csv'. In the section below there are methods to load the data in a consistent format.\n",
    "\n",
    "In general, reserving part of your data as a test set can lead to unpredictable performance- a serendipitous choice of your train or test split could give you a very inaccurate idea of how your classifier performs. That's where k-fold cross validation comes in.\n",
    "\n",
    "In the below method, we'll split the dataset at random into k equal subsections, then iterating on each of our k samples, we'll reserve that sample for testing and use the other k-1 for training. Averaging the results of each fold should give us a more consistent idea of how the classifier is doing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8273381294964028, 0.7913669064748201, 0.8345323741007195, 0.8057553956834532, 0.8273381294964028, 0.8345323741007195, 0.8561151079136691, 0.8273381294964028, 0.8489208633093526, 0.8201438848920863]\n",
      "[0.8103448275862069, 0.7692307692307693, 0.8518518518518519, 0.7301587301587301, 0.8333333333333334, 0.7704918032786885, 0.75, 0.8253968253968254, 0.7936507936507936, 0.8028169014084507]\n",
      "[0.7833333333333333, 0.78125, 0.7540983606557377, 0.8214285714285714, 0.8088235294117647, 0.8392857142857143, 0.9230769230769231, 0.8, 0.8620689655172413, 0.8382352941176471]\n",
      "[[[47, 13], [11, 68]], [[50, 14], [15, 60]], [[46, 15], [8, 70]], [[46, 10], [17, 66]], [[55, 13], [11, 60]], [[47, 9], [14, 69]], [[48, 4], [16, 71]], [[52, 13], [11, 63]], [[50, 8], [13, 68]], [[57, 11], [14, 57]]]\n"
     ]
    }
   ],
   "source": [
    "def load_csv(data_file_path, class_index=-1):\n",
    "    import numpy as np\n",
    "\n",
    "    handle = open(data_file_path, 'r')\n",
    "    contents = handle.read()\n",
    "    handle.close()\n",
    "    rows = contents.split('\\n')\n",
    "    out = np.array([  [float(i) for i in r.split(',')] for r in rows if r ])\n",
    "    classes= list(map(int,  out[:, class_index]))\n",
    "    features = out[:, :class_index]\n",
    "    return features, classes\n",
    "\n",
    "def generate_k_folds(dataset, k):\n",
    "    import random\n",
    "    #TODO this method should return a list of folds,\n",
    "    # where each fold is a tuple like (training_set, test_set)\n",
    "    # where each set is a tuple like (examples, classes)\n",
    "    examples, classes = dataset\n",
    "\n",
    "    folds = []\n",
    "\n",
    "    splitSize = len(classes)/k\n",
    "    splitValue = splitSize * (k-1)\n",
    "    for x in range(0, k):\n",
    "        indexes = list(xrange(len(classes)))\n",
    "        random.shuffle(indexes)\n",
    "\n",
    "        training_examples = []\n",
    "        training_classes = []\n",
    "        test_examples = []\n",
    "        test_classes = []\n",
    "\n",
    "        for y in range(0, splitValue):\n",
    "            indexToTake = indexes[y]\n",
    "            training_examples.append(examples[indexToTake])\n",
    "            training_classes.append(classes[indexToTake])\n",
    "        for z in range(splitValue, len(classes)):\n",
    "            indexToTake = indexes[z]\n",
    "            test_examples.append(examples[indexToTake])\n",
    "            test_classes.append(classes[indexToTake])\n",
    "\n",
    "        training_set = (training_examples, training_classes)\n",
    "        test_set = (test_examples, test_classes)\n",
    "\n",
    "        f = (training_set, test_set)\n",
    "        folds.append(f)\n",
    "    return folds\n",
    "\n",
    "\n",
    "dataset = load_csv('part2_data.csv')\n",
    "ten_folds = generate_k_folds(dataset, 10)\n",
    "\n",
    "#on average your accuracy should be higher than 60%.\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "confusion = []\n",
    "\n",
    "for fold in ten_folds:\n",
    "    train, test = fold\n",
    "    train_features, train_classes = train\n",
    "    test_features, test_classes = test\n",
    "    tree = DecisionTree( )\n",
    "    tree.fit( train_features, train_classes)\n",
    "    output = tree.classify(test_features)\n",
    "\n",
    "    accuracies.append( accuracy(output, test_classes))\n",
    "    precisions.append( precision(output, test_classes))\n",
    "    recalls.append( recall(output, test_classes))\n",
    "    confusion.append( confusion_matrix(output, test_classes))\n",
    "\n",
    "print accuracies\n",
    "print precisions\n",
    "print recalls\n",
    "print confusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Random Forests\n",
    "-------\n",
    "30 pts.\n",
    "\n",
    "The decision boundaries drawn by decision trees are very sharp, and fitting a decision tree of unbounded depth to a list of examples almost inevitably leads to overfitting. In an attempt to decrease the variance of our classifier we're going to use a technique called 'Bootstrap Aggregating' (often abbreviated 'bagging').\n",
    "\n",
    "A Random Forest is a collection of decision trees, built as follows:\n",
    "\n",
    "1) For every tree we're going to build:\n",
    "\n",
    "    a) Subsample the examples provided us (with replacement) in accordance with a provided example subsampling rate.\n",
    "    \n",
    "    b) From the sample in a), choose attributes at random to learn on (in accordance with a provided attribute subsampling rate)\n",
    "    \n",
    "    c) Fit a decision tree to the subsample of data we've chosen (to a certain depth)\n",
    "    \n",
    "Classification for a random forest is then done by taking a majority vote of the classifications yielded by each tree in the forest after it classifies an example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8848920863309353, 0.8776978417266187, 0.841726618705036, 0.7985611510791367, 0.8848920863309353, 0.8705035971223022, 0.8776978417266187, 0.8489208633093526, 0.8201438848920863, 0.8848920863309353]\n",
      "[0.8805970149253731, 0.7796610169491526, 0.7454545454545455, 0.8148148148148148, 0.8852459016393442, 0.8571428571428571, 0.8461538461538461, 0.8857142857142857, 0.7205882352941176, 0.84]\n",
      "[0.8805970149253731, 0.92, 0.8367346938775511, 0.7096774193548387, 0.8571428571428571, 0.8823529411764706, 0.8870967741935484, 0.8266666666666667, 0.8909090909090909, 0.84]\n",
      "[[[59, 8], [8, 64]], [[46, 4], [13, 76]], [[41, 8], [14, 76]], [[44, 18], [10, 67]], [[54, 9], [7, 69]], [[60, 8], [10, 61]], [[55, 7], [10, 67]], [[62, 13], [8, 56]], [[49, 6], [19, 65]], [[42, 8], [8, 81]]]\n"
     ]
    }
   ],
   "source": [
    "class RandomForest():\n",
    "\n",
    "    def __init__(self, num_trees, depth_limit, example_subsample_rate, attr_subsample_rate):\n",
    "        self.trees = []\n",
    "        self.num_trees = num_trees\n",
    "        self.depth_limit = depth_limit\n",
    "        self.example_subsample_rate = example_subsample_rate\n",
    "        self.attr_subsample_rate = attr_subsample_rate\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        import numpy as np\n",
    "        # TODO implement the above algorithm to build a random forest of decision trees\n",
    "        self.trees = []\n",
    "\n",
    "        #index from 0 to length of classes\n",
    "        indexes_features = list(xrange(len(classes)))\n",
    "        indexes_attributes = list(xrange(len(features[0])))\n",
    "\n",
    "        #create B number of trees\n",
    "        for b in range(0, self.num_trees):\n",
    "            \"\"\"\n",
    "            a) Subsample the examples provided us (with replacement) in accordance with a provided example subsampling rate.\n",
    "            \"\"\"\n",
    "            #choose from features, number of samples, and with replacement\n",
    "            numExamplesToSample = int(len(features)*self.example_subsample_rate)\n",
    "            indexesToSampleFeatures = np.random.choice(indexes_features, numExamplesToSample, True)\n",
    "            sampleFeatures = []\n",
    "            sampleClasses = []\n",
    "            for x in range(0, len(indexesToSampleFeatures)):\n",
    "                sampleFeatures.append(features[indexesToSampleFeatures[x]])\n",
    "                sampleClasses.append(classes[indexesToSampleFeatures[x]])\n",
    "\n",
    "            \"\"\"\n",
    "            b) From the sample in a), choose attributes at random to learn on (in accordance with a provided attribute subsampling rate)\n",
    "            \"\"\"\n",
    "            numAttrToSample = int(len(features[0])*self.attr_subsample_rate)\n",
    "            indexesToSampleAttributes = np.random.choice(indexes_attributes, numAttrToSample, False)\n",
    "            notUsedAttributes = list(set(indexes_attributes) - set(indexesToSampleAttributes))\n",
    "            #delete those attributes from sampleFeatures\n",
    "            for x in range(0, len(sampleFeatures)):\n",
    "                sampleFeatures[x] = np.delete(sampleFeatures[x], notUsedAttributes)\n",
    "\n",
    "            \"\"\"\n",
    "            c) Fit a decision tree to the subsample of data we've chosen (to a certain depth)\n",
    "            \"\"\"\n",
    "            t = DecisionTree(self.depth_limit)\n",
    "            t.fit( sampleFeatures, sampleClasses)\n",
    "\n",
    "            #print str(b+1) + \" trees done\"\n",
    "            self.trees.append(t)\n",
    "\n",
    "    def classify(self, features):\n",
    "        # TODO implement classification for a random forest.\n",
    "        classifications = []\n",
    "        for feature in features:\n",
    "            featureClassification = []\n",
    "            for tree in self.trees:\n",
    "                featureClassification.append(tree.root.decide(feature))\n",
    "            if featureClassification.count(1) > featureClassification.count(0):\n",
    "                classifications.append(1)\n",
    "            else:\n",
    "                classifications.append(0)\n",
    "\n",
    "        return classifications\n",
    "\n",
    "#TODO: As with the DecisionTree, evaluate the performance of your RandomForest on the dataset for part 2.\n",
    "# on average your accuracy should be higher than 75%.\n",
    "\n",
    "#  Optimize the parameters of your random forest for accuracy for a forest of 5 trees.\n",
    "# (We'll verify these by training one of your RandomForest instances using these parameters\n",
    "#  and checking the resulting accuracy)\n",
    "\n",
    "#  Fill out the function below to reflect your answer:\n",
    "\n",
    "def ideal_parameters():\n",
    "    ideal_depth_limit = 7 #% of number of attributs, instead of hardcoded depth\n",
    "    ideal_esr = 0.60 #% of samples\n",
    "    ideal_asr = 0.60 #% of attributes\n",
    "    return ideal_depth_limit, ideal_esr, ideal_asr\n",
    "\n",
    "\n",
    "#num_trees, depth_limit, example_subsample_rate, attr_subsample_rate\n",
    "dataset = load_csv('part2_data.csv')\n",
    "ten_folds = generate_k_folds(dataset, 10)\n",
    "\n",
    "#on average your accuracy should be higher than 60%.\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "confusion = []\n",
    "\n",
    "for fold in ten_folds:\n",
    "    train, test = fold\n",
    "    train_features, train_classes = train\n",
    "    test_features, test_classes = test\n",
    "    depth_limit, example_sr, attr_sr = ideal_parameters()\n",
    "    forest = RandomForest(5, depth_limit, example_sr, attr_sr)\n",
    "    forest.fit( train_features, train_classes)\n",
    "    output = forest.classify(test_features)\n",
    "\n",
    "    accuracies.append( accuracy(output, test_classes))\n",
    "    precisions.append( precision(output, test_classes))\n",
    "    recalls.append( recall(output, test_classes))\n",
    "    confusion.append( confusion_matrix(output, test_classes))\n",
    "\n",
    "print accuracies\n",
    "print precisions\n",
    "print recalls\n",
    "print confusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: Challenge!\n",
    "-------\n",
    "10 pts\n",
    "\n",
    "You've been provided with a sample of data from a research dataset in 'challenge_data.pickle'. It is serialized as a tuple of (features, classes). I have reserved a part of the dataset for testing. The classifier that performs most accurately on the holdout set wins (so optimize for accuracy). To get full points for this part of the assignment, you'll need to get at least an average accuracy of 80% on the data you have, and at least an average accuracy of 60% on the holdout set.\n",
    "\n",
    "Ties will be broken by submission time.\n",
    "\n",
    "First place:  +3% on your final grade\n",
    "\n",
    "Second place: +2% on your final grade\n",
    "\n",
    "Third place:  +1% on your final grade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9264069264069265, 0.9307359307359307, 0.9090909090909091, 0.9393939393939394, 0.9393939393939394, 0.9264069264069265, 0.9307359307359307, 0.9264069264069265, 0.9567099567099567, 0.935064935064935]\n",
      "[0.9382716049382716, 0.9540229885057471, 0.9222222222222223, 0.9382716049382716, 0.9042553191489362, 0.9326923076923077, 0.8936170212765957, 0.9101123595505618, 0.9523809523809523, 0.9230769230769231]\n",
      "[0.8636363636363636, 0.8736842105263158, 0.8556701030927835, 0.8941176470588236, 0.9444444444444444, 0.9065420560747663, 0.9333333333333333, 0.9, 0.9302325581395349, 0.9130434782608695]\n",
      "[[[76, 12], [5, 138]], [[83, 12], [4, 132]], [[83, 14], [7, 127]], [[76, 9], [5, 141]], [[85, 5], [9, 132]], [[97, 10], [7, 117]], [[84, 6], [10, 131]], [[81, 9], [8, 133]], [[80, 6], [4, 141]], [[84, 8], [7, 132]]]\n",
      "Mean accuracy: 0.932034632035\n"
     ]
    }
   ],
   "source": [
    "class ChallengeClassifier():\n",
    "    def __init__(self, num_trees=5, depth_limit=0.2, example_subsample_rate=1.0, attr_subsample_rate=1.0):\n",
    "        # initialize whatever parameters you may need here-\n",
    "        # this method will be called without parameters\n",
    "        # so if you add any to make parameter sweeps easier, provide defaults\n",
    "        self.trees = []\n",
    "        self.num_trees = num_trees\n",
    "        self.depth_limit = depth_limit\n",
    "        self.example_subsample_rate = example_subsample_rate\n",
    "        self.attr_subsample_rate = attr_subsample_rate\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        import numpy as np\n",
    "        # TODO implement the above algorithm to build a random forest of decision trees\n",
    "        self.trees = []\n",
    "\n",
    "        #index from 0 to length of classes\n",
    "        indexes_features = list(xrange(len(classes)))\n",
    "        indexes_attributes = list(xrange(len(features[0])))\n",
    "\n",
    "        #create B number of trees\n",
    "        for b in range(0, self.num_trees):\n",
    "            \"\"\"\n",
    "            a) Subsample the examples provided us (with replacement) in accordance with a provided example subsampling rate.\n",
    "            \"\"\"\n",
    "            #choose from features, number of samples, and with replacement\n",
    "            numExamplesToSample = int(len(features)*self.example_subsample_rate)\n",
    "            indexesToSampleFeatures = np.random.choice(indexes_features, numExamplesToSample, True)\n",
    "            sampleFeatures = []\n",
    "            sampleClasses = []\n",
    "            for x in range(0, len(indexesToSampleFeatures)):\n",
    "                sampleFeatures.append(features[indexesToSampleFeatures[x]])\n",
    "                sampleClasses.append(classes[indexesToSampleFeatures[x]])\n",
    "\n",
    "            \"\"\"\n",
    "            b) From the sample in a), choose attributes at random to learn on (in accordance with a provided attribute subsampling rate)\n",
    "            \"\"\"\n",
    "            numAttrToSample = int(len(features[0])*self.attr_subsample_rate)\n",
    "            indexesToSampleAttributes = np.random.choice(indexes_attributes, numAttrToSample, False)\n",
    "            notUsedAttributes = list(set(indexes_attributes) - set(indexesToSampleAttributes))\n",
    "            #delete those attributes from sampleFeatures\n",
    "            for x in range(0, len(sampleFeatures)):\n",
    "                sampleFeatures[x] = np.delete(sampleFeatures[x], notUsedAttributes)\n",
    "\n",
    "            \"\"\"\n",
    "            c) Fit a decision tree to the subsample of data we've chosen (to a certain depth)\n",
    "            \"\"\"\n",
    "            t = FancyDecisionTree(self.depth_limit)#DecisionTree(7)#FancyDecisionTree(self.depth_limit) #Fancy\n",
    "            t.fit( sampleFeatures, sampleClasses)\n",
    "\n",
    "            #print str(b+1) + \" trees done\"\n",
    "            self.trees.append(t)\n",
    "\n",
    "    def classify(self, features):\n",
    "        # classify each feature in features as either 0 or 1.\n",
    "        classifications = []\n",
    "        for feature in features:\n",
    "            featureClassification = []\n",
    "            for tree in self.trees:\n",
    "                featureClassification.append(tree.root.decide(feature))\n",
    "            if featureClassification.count(1) > featureClassification.count(0):\n",
    "                classifications.append(1)\n",
    "            else:\n",
    "                classifications.append(0)\n",
    "        return classifications\n",
    "\n",
    "class FancyDecisionTree():\n",
    "    def __init__(self, depth_limit=float('inf')):\n",
    "        self.root = None\n",
    "        self.depth_limit = depth_limit\n",
    "\n",
    "    def fit(self, features, classes):\n",
    "        self.root = self.__build_tree__(features, classes)\n",
    "\n",
    "    def __build_tree__(self, features, classes, depth=0):\n",
    "        import copy\n",
    "        import numpy as np\n",
    "        #print \"depth is: \" + str(depth)\n",
    "        #print \"features: \" + str(features)\n",
    "        #print \"classes: \" + str(classes)\n",
    "\n",
    "        #print \"features size: \" + str(len(features)) + \"x\" + str(len(features[0]))\n",
    "        #print \"classes size: \" + str(len(classes))\n",
    "\n",
    "        TDNode = DecisionNode(None, None, None, 1)\n",
    "        FDNode = DecisionNode(None, None, None, 0)\n",
    "        \"\"\"\n",
    "        1) Check for base cases:\n",
    "            a)If all elements of a list are of the same class, return a leaf node with the appropriate class label.\n",
    "            b)If a specified depth limit is reached, return a leaf labeled with the most frequent class.\n",
    "            My addition to algo: If there are no attributes left, return most frequent class\n",
    "        \"\"\"\n",
    "        if classes == [0]*len(classes):\n",
    "            return FDNode\n",
    "        elif classes == [1]*len(classes):\n",
    "            return TDNode\n",
    "\n",
    "        #depth percent of attributes\n",
    "        if len(features[0]) == 0 or (self.depth_limit != float('inf') and depth >= int(len(features[0])*self.depth_limit) and (classes.count(1) > 3*classes.count(0) or classes.count(0) > 3*classes.count(1))):\n",
    "            if classes.count(1) > classes.count(0):\n",
    "                return TDNode\n",
    "            else:\n",
    "                return FDNode\n",
    "\n",
    "        \"\"\"\n",
    "        2) For each attribute alpha: evaluate the normalized information gain gained by splitting on alpha\n",
    "        \"\"\"\n",
    "        bestAttributeIndex = 0\n",
    "        bestAttributeIndexSplitValue = 0\n",
    "        bestInfoGain = 0.0\n",
    "        #go through each attribute\n",
    "        for x in range(0, len(features[0])):\n",
    "            #attribute column\n",
    "            a = [row[x] for row in features]\n",
    "\n",
    "            uniqueAValues = list(set(a))\n",
    "            #testing using just middle value\n",
    "            #uniqueAValues = [uniqueAValues[len(uniqueAValues)/2]]\n",
    "            #testing getting randomly some values\n",
    "            uniqueAValues = np.random.choice(uniqueAValues, int(len(uniqueAValues)*0.15) + 1, False)\n",
    "            for u in uniqueAValues:\n",
    "                #split on value\n",
    "                #all final true false values where attrubute a was <> each unique value\n",
    "                curr_classes = []\n",
    "                for y in range(0, len(a)):\n",
    "                    if a[y] > u:\n",
    "                        curr_classes.append(classes[y])\n",
    "\n",
    "                infoGain = information_gain(classes, curr_classes)\n",
    "                if infoGain > bestInfoGain:\n",
    "                    bestAttributeIndex = x\n",
    "                    bestAttributeIndexSplitValue = u\n",
    "                    bestInfoGain = infoGain\n",
    "\n",
    "        \"\"\"\n",
    "        3) Let alpha_best be the attribute with the highest normalized information gain\n",
    "        \"\"\"\n",
    "        alpha_best = [row[bestAttributeIndex] for row in features]\n",
    "\n",
    "        \"\"\"\n",
    "        4) Create a decision node that splits on alpha_best\n",
    "        \"\"\"\n",
    "        #remove best attribute column\n",
    "        new_features = copy.deepcopy(features)\n",
    "        #testing without removing attribute column... in submission I removed column\n",
    "        \"\"\"\n",
    "        for x in range(0, len(new_features)):\n",
    "            new_features[x] = np.delete(new_features[x], bestAttributeIndex)\n",
    "        \"\"\"\n",
    "\n",
    "        l_features = []\n",
    "        l_classes = []\n",
    "        r_features = []\n",
    "        r_classes = []\n",
    "        #split cases that will go left and ritht, left will cases where alpha_best will be > than best split value\n",
    "        for x in range(0, len(alpha_best)):\n",
    "            if alpha_best[x] > bestAttributeIndexSplitValue:\n",
    "                l_features.append(new_features[x])\n",
    "                l_classes.append(classes[x])\n",
    "            else:\n",
    "                r_features.append(new_features[x])\n",
    "                r_classes.append(classes[x])\n",
    "        #print \"basisv: \" + str(bestAttributeIndexSplitValue)\n",
    "        def RootDFunction(feature):\n",
    "            if feature[bestAttributeIndex] > bestAttributeIndexSplitValue:\n",
    "                return 1\n",
    "            else:\n",
    "                return 0\n",
    "\n",
    "        #print \"about to build a tree with l features: \" + str(l_features)\n",
    "        #print \"about to build a tree with l class: \" + str(l_classes)\n",
    "        #print \"about to build a tree with r reatures: \" + str(r_features)\n",
    "        #print \"about to build a tree with r class: \" + str(r_classes)\n",
    "        #dn left, right, decisionfunction, class\n",
    "        root_node = DecisionNode(self.__build_tree__(l_features, l_classes, depth+1), self.__build_tree__(r_features, r_classes, depth+1), RootDFunction)\n",
    "\n",
    "        \"\"\"\n",
    "        5) Recur on the sublists obtained by splitting on alpha_best, and add those nodes as children of node\n",
    "        \"\"\"\n",
    "        # already doing that above\n",
    "\n",
    "        return root_node\n",
    "\n",
    "    def classify(self, features):\n",
    "        # Your output should be a list of class labels (either 0 or 1)\n",
    "        return [self.root.decide(feature) for feature in features]\n",
    "\n",
    "#Validate\n",
    "import pickle\n",
    "dataset = pickle.load(open('challenge_data.pickle', 'rb'))\n",
    "ten_folds = generate_k_folds(dataset, 10)\n",
    "\n",
    "#on average your accuracy should be higher than 60%.\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "confusion = []\n",
    "\n",
    "for fold in ten_folds:\n",
    "    train, test = fold\n",
    "    train_features, train_classes = train\n",
    "    test_features, test_classes = test\n",
    "    forest = ChallengeClassifier()\n",
    "    forest.fit( train_features, train_classes)\n",
    "    output = forest.classify(test_features)\n",
    "    accuracies.append( accuracy(output, test_classes))\n",
    "    precisions.append( precision(output, test_classes))\n",
    "    recalls.append( recall(output, test_classes))\n",
    "    confusion.append( confusion_matrix(output, test_classes))\n",
    "\n",
    "print accuracies\n",
    "print precisions\n",
    "print recalls\n",
    "print confusion\n",
    "\n",
    "import numpy as np\n",
    "print \"Mean accuracy: \" + str(np.mean(accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
